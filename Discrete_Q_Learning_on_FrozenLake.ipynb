{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilfaizal01/Reinforcement-Learning/blob/main/Discrete_Q_Learning_on_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "TvOxEUM3-bMu"
      },
      "id": "TvOxEUM3-bMu"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "billion-funds",
      "metadata": {
        "id": "billion-funds"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "from IPython.display import clear_output\n",
        "import time  # slow the game down a little bit\n",
        "import gym\n",
        "import numpy as np  # used for all kinds of matrix / vector operations\n",
        "import matplotlib.pyplot as plt  # for plotting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SFFF       (S: starting point, safe)\n",
        "# FHFH       (F: frozen surface, safe)\n",
        "# FFFH       (H: hole, fall to your doom)\n",
        "# HFFG       (G: goal, where the frisbee is located)"
      ],
      "metadata": {
        "id": "NAFqcUty-lH7"
      },
      "id": "NAFqcUty-lH7",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e69704fa",
      "metadata": {
        "id": "e69704fa"
      },
      "source": [
        "# **Registering a new environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "3cdd8751",
      "metadata": {
        "id": "3cdd8751"
      },
      "outputs": [],
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "  id='FrozenLakeNotSlippery-v3',\n",
        "  entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "  kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "  max_episode_steps=100,\n",
        "  reward_threshold=.8196,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0e9f80",
      "metadata": {
        "id": "4f0e9f80"
      },
      "source": [
        "## **Testing environment with some random actions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "fd071ecc",
      "metadata": {
        "id": "fd071ecc"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLakeNotSlippery-v3')\n",
        "\n",
        "# Reset the environment to its initial state\n",
        "state = env.reset()\n",
        "\n",
        "# Define the number of steps for the agent to take\n",
        "num_steps = 20"
      ],
      "metadata": {
        "id": "2whWS7Z6BYX2"
      },
      "id": "2whWS7Z6BYX2",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(num_steps):\n",
        "  action = env.action_space.sample()\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Episode finished\")\n",
        "    state = env.reset()\n",
        "  else:\n",
        "    state = next_state\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "X_1RzImg_t-K",
        "outputId": "3c5a2671-de24-4ae4-ad04-7c30c93ca61c"
      },
      "id": "X_1RzImg_t-K",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished\n",
            "Episode finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "2ec3a21f",
      "metadata": {
        "id": "2ec3a21f"
      },
      "outputs": [],
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14e800e",
      "metadata": {
        "id": "c14e800e"
      },
      "source": [
        "## **Initial Q-Table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "latter-header",
      "metadata": {
        "id": "latter-header"
      },
      "outputs": [],
      "source": [
        "# Start with very small values for all our Q(s,a)\n",
        "q_table = np.zeros([state_size, action_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "9c71f19e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "9c71f19e",
        "outputId": "7992740d-d6fd-44ba-ad56-fc1795e8e0a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "45fe3323",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "45fe3323",
        "outputId": "9c4302e4-92aa-4e12-feea-b30efe04800b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "q_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameters**"
      ],
      "metadata": {
        "id": "btBCTyuWC9vF"
      },
      "id": "btBCTyuWC9vF"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "8666bae0",
      "metadata": {
        "id": "8666bae0"
      },
      "outputs": [],
      "source": [
        "EPOCHS=20000  # number of epochs/episodes to train for\n",
        "ALPHA = 0.8 # aka the learning rate\n",
        "GAMMA = 0.95 # aka the discount rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "cd70a2a4",
      "metadata": {
        "id": "cd70a2a4"
      },
      "outputs": [],
      "source": [
        "# Exploration vs. Exploitation parameters\n",
        "epsilon = 1.0 # Exploration rate\n",
        "max_epsilon = 1.0 # Exploration probability at start\n",
        "min_epsilon = 0.01 # Minimum exploration probability\n",
        "decay_rate = 0.001 # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d29c33",
      "metadata": {
        "id": "31d29c33"
      },
      "source": [
        "# **Q-Table Update Functions Methodology**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "38439bea",
      "metadata": {
        "id": "38439bea"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
        "  random_number = np.random.random()\n",
        "  # EXPLOITATION, USE BEST Q(s,a) Value\n",
        "  if random_number > epsilon:\n",
        "    # Action row for a particular state\n",
        "    state_row = q_table[discrete_state,:]\n",
        "    # Index of highest action for state\n",
        "    # Recall action is mapped to index (e.g. 0=LEFT, 1=DOWN, etc..)\n",
        "    action = np.argmax(state_row)\n",
        "  # EXPLORATION, USE A RANDOM ACTION\n",
        "  else:\n",
        "    # Return a random 0,1,2,3 action\n",
        "    action = env.action_space.sample()\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0a4e89",
      "metadata": {
        "id": "6d0a4e89"
      },
      "source": [
        "## **FUNCTION FOR Q_VALUE COMPUTATION**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "17ec3764",
      "metadata": {
        "id": "17ec3764"
      },
      "outputs": [],
      "source": [
        "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
        "  return old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e05888",
      "metadata": {
        "id": "14e05888"
      },
      "source": [
        "## **FUNCTION TO REDUCE EPSILON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "74c791f9",
      "metadata": {
        "id": "74c791f9"
      },
      "outputs": [],
      "source": [
        "def reduce_epsilon(epsilon,epoch):\n",
        "  return min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c42807",
      "metadata": {
        "id": "68c42807"
      },
      "source": [
        "# **Training of Agent and Updating Q-Table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "5d86c124",
      "metadata": {
        "id": "5d86c124"
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([state_size, action_size])\n",
        "total_reward = 0\n",
        "epsilon = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "cdb1e407",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cdb1e407",
        "outputId": "4142e12b-eb45-40c0-a48e-9f8ca623e21e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "080df6d4",
      "metadata": {
        "id": "080df6d4"
      },
      "outputs": [],
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# Play 20k games\n",
        "for episode in range(EPOCHS):\n",
        "  # Reset the environment\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  total_rewards = 0\n",
        "\n",
        "  while not done:\n",
        "    action = epsilon_greedy_action_selection(epsilon,q_table, state)\n",
        "    # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    # Look up current/old qtable value Q(s_t,a_t)\n",
        "    old_q_value =  q_table[state,action]\n",
        "    # Get the next optimal Q-Value\n",
        "    next_optimal_q_value = np.max(q_table[new_state, :])\n",
        "    # Compute next q value\n",
        "    next_q = compute_next_q_value(old_q_value, reward, next_optimal_q_value)\n",
        "    # Update Q Table\n",
        "    q_table[state,action] = next_q\n",
        "    total_rewards = total_rewards + reward\n",
        "    # Our new state is state\n",
        "    state = new_state\n",
        "\n",
        "  episode += 1\n",
        "  # Reduce epsilon - less exploration needed\n",
        "  epsilon = reduce_epsilon(epsilon,episode)\n",
        "  rewards.append(total_rewards)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ab364621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ab364621",
        "outputId": "c4dd0af6-41d3-4f48-be80-8343aac353a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.73509189, 0.77378094, 0.6983373 , 0.73509189],\n",
              "       [0.73509189, 0.        , 0.66268191, 0.69833551],\n",
              "       [0.69826931, 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
              "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
              "       [0.857375  , 0.95      , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
              "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "q_table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e33b897",
      "metadata": {
        "id": "4e33b897"
      },
      "source": [
        "# **Using Learned Q Table Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "28938392",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "28938392",
        "outputId": "c83a2839-39df-4070-ec5a-a08e59bac612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n",
            "Episode finished\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLakeNotSlippery-v3')\n",
        "\n",
        "state = env.reset()\n",
        "rewards = 0\n",
        "\n",
        "for _ in range(30):\n",
        "  action = np.argmax(q_table[state])  # and chose action from the Q-Table\n",
        "  state, reward, done, info = env.step(action) # Finally perform the action\n",
        "  env.render(mode='human')\n",
        "  if done:\n",
        "    print(\"Episode finished\")\n",
        "    state = env.reset()\n",
        "  else:\n",
        "    state = next_state\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}